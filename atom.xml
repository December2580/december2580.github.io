<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小宅的博客</title>
  
  <subtitle>你想批评指点四周风景，你首先要爬上屋顶。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://december2580.github.io/"/>
  <updated>2019-12-06T06:41:24.839Z</updated>
  <id>https://december2580.github.io/</id>
  
  <author>
    <name>宅</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>网络相关学习笔记（一）</title>
    <link href="https://december2580.github.io/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"/>
    <id>https://december2580.github.io/网络相关学习笔记.html</id>
    <published>2019-12-06T06:28:49.000Z</published>
    <updated>2019-12-06T06:41:24.839Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近在看计算机网络的一些内容，看完着实觉得自己当初学习这门课程的时候，划水划得厉害，一点都没有深入的去理解和思考，虽然现在也还是不够深入….随手整理了一些小的知识点，这里虽然标题带了个一，但也有可能是最后一篇，感觉项目好像做不下去了….虽然还没开始，唉。</p><ol><li><p>路由器实现了网关的功能，路由器基于ip协议。</p></li><li><p>以太网协议实现了局域网内部的计算机之间的通信，但是多个局域网之间无能为力，ip协议实现了局域网之间的通信，但是可能存在丢包的现象（缓存满了，新的数据包丢失），而且通信是主机对主机，tcp保证了通信的完整性和可靠性，防止丢包，通信是进程对进程。</p></li><li><p>tcp数据包的编号，SEQ。第一个包的编号是一定基础上生成的一个随机数（还记得当初阿里面试的时候被问过这个题，当时依稀记得在哪里看过是个随机数），假设为1，第一个包的长度为100，那么第二个包的编号为101，所以由一个数据包可以知道当前数据包和下一个数据包的编号，接收方也就便于还原。</p></li><li><p>确认消息ACK。ACK包含两部分信息，一是期待要收到的下一个数据包的编号，比如当发送方seq=1,length=100，此时接收方ack=101，对前面100字节进行确认，并期待收到101开头的数据包；二是当前接收方接收窗口的剩余容量。</p></li><li><p>重发机制：由上述我们知道，ACK会包含期待的下一个数据包的编号，当接受方没有收到期待的编号的数据包，会停在这个编号，不会增加，当发送方发现连续收到了三个相同的ACK，或者超时没有收到ACK，会重发ACK期待的数据包。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;&amp;emsp;最近在看计算机网络的一些内容，看完着实觉得自己当初学习这门课程的时候，划水划得厉害，一点都没有深入的去理解和思考，虽然现在也还是不够深入….随手整理了一些小的知识点，这里虽然标题带了个一，但也有可能是最后一篇，感觉项目好像做不下去了….虽然还没开始，
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://december2580.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="https://december2580.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>从在浏览器中输入www.baidu.com到浏览器显示百度首页发生了什么</title>
    <link href="https://december2580.github.io/%E4%BB%8E%E5%9C%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E8%BE%93%E5%85%A5www-baidu-com%E5%88%B0%E6%B5%8F%E8%A7%88%E5%99%A8%E6%98%BE%E7%A4%BA%E7%99%BE%E5%BA%A6%E9%A6%96%E9%A1%B5%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88.html"/>
    <id>https://december2580.github.io/从在浏览器中输入www-baidu-com到浏览器显示百度首页发生了什么.html</id>
    <published>2019-12-02T01:27:53.000Z</published>
    <updated>2019-12-04T13:52:52.942Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近在看网络编程的一些东西，偶然看到有一篇文章将“在浏览器中输入 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 后，浏览器和服务器之间的通信过程”作为一个举例，想到了一道比较经典的面试题（朋友在面阿里后端一面的时候问过），就是我们在浏览器输入 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 点击enter后，到浏览器显示百度的主页，这个过程到底发生了什么。<br>&emsp;&emsp;ps：笔者也是作为一个学习者的角度来按照自己的理解进行一些解读。<br>&emsp;&emsp;整个的环境就是，我们平时的一个局域网中的一台电脑，来访问百度的主页，主要过程如下：</p><ol><li>打开浏览器在地址栏输入 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> ，说明浏览器想要给百度发送一个网页请求的数据包。</li><li>浏览器通过DNS域名解析服务器（在本地）将域名解析为对应的IP地址。</li><li>判断这个ip是不是和本机在同一子网，这里需要用到子网掩码，本机分别将两个ip地址（本机ip和百度ip）做一个and运算，看结果是否是一样，是-&gt;局域网内通信，直接tcp连接，不是-&gt;跨局域网通信，通过网关进行连接。</li><li>比较发现不属于同一子网，因此数据包需要通过网关进行转发（即本机将请求转发给网关，网关转发给百度服务器，百度服务器的响应结果返回给网关，网关再转发给本机）。</li><li>通过ARP协议获得 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 的ip对应的mac地址，通过mac地址和ip地址就确定了要建立通信的对象。</li><li>网关和百度服务器三次握手建立TCP连接。</li><li>百度服务器收到请求，返回网页的html。</li><li>网关将响应结果转发给相应的主机的浏览器。</li><li>浏览器解析html，请求网页中需要的各种资源。</li><li>浏览器获得请求的资源，对html进行渲染。</li><li>显示百度的首页。</li></ol><p>&emsp;&emsp;大致上是这么个过程。<br>&emsp;&emsp;这里需要解释一下一个问题：<br><strong>&emsp;&emsp;为什么区分私网（局域网）和公网（广域网）以及网关怎么知道将消息转发给哪一台主机？</strong><br>&emsp;&emsp;首先公网IP是有限的，而且是收费的，全球这么多设备要上网，每台设备一个IP肯定是不够的，因此在一个局域网中，可以多个设备共用一个公网IP，每台设备又有一个子网IP。这就相当于，全球每个人需要住一个房子是不够的，我们可以安排一家人住一个房子，每个人住一个房间，他们共享一个家庭住址，每个人又有自己的房间地址。在局域网内通信可以直接使用设备子网IP，因为这个IP是大家都知道的，就像房子中你知道上楼左转是哥哥的房间，右转是姐姐的房间一样，但是不同子网之间是不能使用这个IP的，因为这个左转和右转，仅仅是局限于你们家的房子，别人家的房子构造是不一样的，这时候我们就需要通过网关来进行通信，你可以理解成房子与房子之间的通信，使用住址进行通信，假设只有爸爸知道房子的住址，可以和其他房子进行通信，这样你就可以通过爸爸发消息给你的好朋友小明，(这里的你和小明分别是两个子网中的两台主机，而爸爸相当于网关)，这样我们就实现了跨子网（房子）的通信，但是你把消息发给小明，小明给你回信之后，爸爸怎么知道这个信是给谁的呢（也就是网关怎么知道这个消息应该转发给哪一台主机），这里使用端口映射来进行区分。我们只需要将设备ip+端口，映射为网关ip+端口即可。<br>&emsp;&emsp;举例：</p><table><thead><tr><th>设备名</th><th>IP</th></tr></thead><tbody><tr><td>网关</td><td>192.168.1.1</td></tr><tr><td>设备A</td><td>192.168.1.2</td></tr><tr><td>设备B</td><td>192.168.1.3</td></tr><tr><td>设备C</td><td>192.168.1.4</td></tr></tbody></table><p>&emsp;&emsp;端口映射后：</p><table><thead><tr><th>源ip:端口</th><th>映射之后的ip:端口</th></tr></thead><tbody><tr><td>A：192.168.1.2:60</td><td>192.168.1.1:80</td></tr><tr><td>B：192.168.1.3:60</td><td>192.168.1.1:90</td></tr><tr><td>C：192.168.1.4:60</td><td>192.168.1.1:100</td></tr></tbody></table><p>&emsp;&emsp;这样我们就可以知道，网关80端口来的消息要转发给设备A的60端口，依次类推。也就实现了局域网共用一个公网IP的方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;&amp;emsp;最近在看网络编程的一些东西，偶然看到有一篇文章将“在浏览器中输入 &lt;a href=&quot;http://www.baidu.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.baidu.com&lt;/a&gt; 后，浏览器和服务器之间的
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://december2580.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="https://december2580.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy 爬虫实战</title>
    <link href="https://december2580.github.io/Scrapy-%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98.html"/>
    <id>https://december2580.github.io/Scrapy-爬虫实战.html</id>
    <published>2019-11-15T12:16:36.000Z</published>
    <updated>2019-11-15T13:51:58.482Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近在看爬虫，跟着Scrapy的官方文档实例走了一遍，等自己想爬别的网站的时候，发现还是有点问题，于是网上找了个教程，自己跟着做了一遍，然后换了一个网站，爬一下别的数据，现在做一下记录，由于也是刚刚接触，所以在一些使用上可能还存在误区，欢迎大家批评指正。<br>&emsp;&emsp;<a href="http://www.weather.com.cn/weather/101250101.shtml" target="_blank" rel="noopener">http://www.weather.com.cn/weather/101250101.shtml</a><br>&emsp;&emsp;这是我们爬取信息的网址，爬取长沙未来七天的天气情况。</p><p>&emsp;&emsp;环境：windows10，python3.6，scrapy1.8，anaconda，chorme</p><h2 id="创建scrapy项目并生成相应的文件"><a href="#创建scrapy项目并生成相应的文件" class="headerlink" title="创建scrapy项目并生成相应的文件"></a>创建scrapy项目并生成相应的文件</h2><p>&emsp;&emsp;首先选择一个你的项目位置，然后在该位置下打开cmd，执行 <strong>scrapy start testSpider</strong> ,初始化一个scrapy项目，其中最后一个字段是项目名称，效果如下：<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_1.png" alt=""></p><p>&emsp;&emsp;在PyCharm中可以查看一下目录结构：<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_2.png" alt=""></p><p>&emsp;&emsp;这些文件的主要作用在官方文档里都有介绍。<br>&emsp;&emsp;然后我们回到cmd，根据提示进行操作，cd testSpider 切换到项目的文件夹下，执行 <strong>scrapy genspider example example.com</strong>，这里example是你的爬虫的name，是他的唯一标示，不同的爬虫的name必须不同，example.com是允许爬取的域。<br>&emsp;&emsp;我们执行&emsp;<strong>scrapy genspider weather weather.com.cn</strong>&emsp;创建一个爬虫的执行文件。执行完之后可以PyCharm中看到在spiders的文件夹下生成了一个weather.py文件：<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_3.png" alt=""></p><p>&emsp;&emsp;文件包含一些自动生成的代码。</p><h2 id="分析需要爬取的网页并编码"><a href="#分析需要爬取的网页并编码" class="headerlink" title="分析需要爬取的网页并编码"></a>分析需要爬取的网页并编码</h2><p>&emsp;&emsp;我们打开那个网址，ctrl+shift+i，检查，一步一步找到我们需要的信息，最终发现，这些天气的信息都在一个如图所示的位置。<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_4.png" alt=""></p><p>&emsp;&emsp;找到这个位置之后，我们点开一天的，信息如下图所示，因为我是在晚上写的这篇博客，看的又是第一天，所以只有最低温度，没有最高温度，我们提取 天，天气，最低温和最高温这4项信息。<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_5.png" alt=""></p><p>&emsp;&emsp;这时候，我们需要打开项目中的items.py，写一个类来保存（封装）我们的这些数据，如下图所示。<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_6.png" alt=""></p><p>&emsp;&emsp;下面需要我们的爬虫登场了，打开spider下的weather.py，我们下面使用BeautifulSoup来进行数据的提取。我直接贴上代码喽。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> weatherSpider.items <span class="keyword">import</span> WeatherItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeahterSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'weather'</span><span class="comment">#spider的名字，唯一标志</span></span><br><span class="line">    allowed_domains = [<span class="string">'weather.com.cn'</span>]<span class="comment">#允许爬取的域</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.weather.com.cn/weather/101250101.shtml'</span>]<span class="comment">#开始爬取的起点</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        html = response.text<span class="comment">#获得爬到的网页的HTML代码</span></span><br><span class="line"></span><br><span class="line">        soup = BeautifulSoup(html,<span class="string">'lxml'</span>)<span class="comment">#创建一个soup对象</span></span><br><span class="line">        weather = WeatherItem()<span class="comment">#创建一个weatherItem对象，存储爬取的信息</span></span><br><span class="line">        div_7d = soup.find(<span class="string">'div'</span>,class_=<span class="string">'c7d'</span>)<span class="comment">#找到class为c7d的标签</span></span><br><span class="line">        <span class="keyword">if</span> div_7d <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#如果存在</span></span><br><span class="line">            ul = div_7d.find(<span class="string">'ul'</span>)<span class="comment">#逐步缩小范围</span></span><br><span class="line">            <span class="keyword">if</span> ul <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> li <span class="keyword">in</span> ul.find_all(<span class="string">'li'</span>):</span><br><span class="line">                    <span class="keyword">if</span> li <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        weather[<span class="string">'day'</span>] = li.find(<span class="string">'h1'</span>).get_text()<span class="comment">#取标签包含的数据</span></span><br><span class="line">                        weather[<span class="string">'wea'</span>] = li.find(<span class="string">'p'</span>).get_text()</span><br><span class="line">                        span = li.find(<span class="string">'span'</span>)</span><br><span class="line">                        <span class="keyword">if</span> span <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                            weather[<span class="string">'highest'</span>] = span.get_text()</span><br><span class="line">                        weather[<span class="string">'lowest'</span>] = li.find(<span class="string">'i'</span>).get_text()</span><br><span class="line">                        print(weather)</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;整体的思想就是一步一步缩小范围，逐步获得我们需要的数据。</p><h2 id="entryPoint-py-便捷启动"><a href="#entryPoint-py-便捷启动" class="headerlink" title="entryPoint.py 便捷启动"></a>entryPoint.py 便捷启动</h2><p>&emsp;&emsp;编写完上面的代码，在scrapy项目的根目录下执行&emsp;<strong>scrapy crawl weather</strong>&emsp;就能为了能够运行爬虫程序了，但是为了更加方便的启动爬虫，我们可以在项目根目录下编写一个entrypoint.py文件，直接在PyCharm中启动爬虫，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line">execute([<span class="string">'scrapy'</span>,<span class="string">'crawl'</span>,<span class="string">'weather'</span>])</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;执行之后就能够看到爬取的数据的结果了：<br><img src="http://q0uiq01q3.bkt.clouddn.com/3_7.png" alt=""></p><p>&emsp;&emsp;到这，这个简单的scrapy爬虫实战就结束了….确实很简单，但也会有成就感。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>&emsp;&emsp;最后补充一点，菜鸡用markdown的时候，#后面忘记加空格，导致标题显示不出来，嘿嘿嘿，抽自己一下。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;&amp;emsp;最近在看爬虫，跟着Scrapy的官方文档实例走了一遍，等自己想爬别的网站的时候，发现还是有点问题，于是网上找了个教程，自己跟着做了一遍，然后换了一个网站，爬一下别的数据，现在做一下记录，由于也是刚刚接触，所以在一些使用上可能还存在误区，欢迎大家批评指
      
    
    </summary>
    
    
      <category term="学习" scheme="https://december2580.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Scrapy" scheme="https://december2580.github.io/tags/Scrapy/"/>
    
      <category term="爬虫" scheme="https://december2580.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>git@git.coding.net: Permission denied (publickey)的解决办法</title>
    <link href="https://december2580.github.io/git-git-coding-net-Permission-denied-publickey-%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.html"/>
    <id>https://december2580.github.io/git-git-coding-net-Permission-denied-publickey-的解决办法.html</id>
    <published>2019-11-12T06:29:16.000Z</published>
    <updated>2019-11-12T08:05:17.350Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;报错如下：<br><img src="http://q0uiq01q3.bkt.clouddn.com/pd.png" alt=""> </p><p>&emsp;&emsp;但是我已经在站点配置文件配置好，并且已经在coding的项目中添加了公钥，但是请求连接时还是出现了问题。<br>&emsp;&emsp;解决办法：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>执行   eval `ssh-agent`</span><br><span class="line"><span class="number">2.</span>执行   ssh-add ~/.ssh/rsa</span><br><span class="line"><span class="number">3.</span>执行   ssh-add –l</span><br><span class="line"><span class="number">4.</span>重新尝试连接  ssh -T <span class="symbol">git@</span>e.coding.net</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;之前一直使用的 ssh -T <a href="mailto:git@git.coding.net" target="_blank" rel="noopener">git@git.coding.net</a> 结果执行完上面三个指令还是连接不上，后来在Coding的官方文档中看到这个<br><img src="http://q0uiq01q3.bkt.clouddn.com/coding.png" alt=""></p><p>&emsp;&emsp;原来是连接指令错误….<br>&emsp;&emsp;这样就成功连接了。</p><p><img src="http://q0uiq01q3.bkt.clouddn.com/success.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;&amp;emsp;报错如下：&lt;br&gt;&lt;img src=&quot;http://q0uiq01q3.bkt.clouddn.com/pd.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;但是我已经在站点配置文件配置好，并且已经在coding的项目中添加了公钥，
      
    
    </summary>
    
    
      <category term="hexo博客搭建" scheme="https://december2580.github.io/categories/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="hexo" scheme="https://december2580.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>hexo搭建博客时遇到的问题</title>
    <link href="https://december2580.github.io/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.html"/>
    <id>https://december2580.github.io/hexo搭建博客时遇到的问题.html</id>
    <published>2019-11-09T09:03:02.000Z</published>
    <updated>2019-11-09T12:03:09.246Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo-g-执行后不生成index-html"><a href="#hexo-g-执行后不生成index-html" class="headerlink" title="hexo g 执行后不生成index.html"></a>hexo g 执行后不生成index.html</h2><p>&emsp;&emsp;这里也只是提出几种可能性:</p><ul><li>我当时执行 npm install 之后重新执行一遍就可以了</li><li>朋友发现自己多包含了一个头文件</li></ul><h2 id="修改背景图片"><a href="#修改背景图片" class="headerlink" title="修改背景图片"></a>修改背景图片</h2><p>&emsp;&emsp;next主题更新后，就没有了custom.styl文件，当时网上很多教程都是使用这个文件设置背景图片，但是新版本的next也提供了用户自定义的文件，首先需要在next主题的配置文件中启用：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">custom_file_path:</span></span><br><span class="line">  <span class="comment">#head: source/_data/head.swig</span></span><br><span class="line">  <span class="comment">#header: source/_data/header.swig</span></span><br><span class="line">  <span class="comment">#sidebar: source/_data/sidebar.swig</span></span><br><span class="line">  <span class="comment">#postMeta: source/_data/post-meta.swig</span></span><br><span class="line">  <span class="comment">#postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line">  <span class="comment">#footer: source/_data/footer.swig</span></span><br><span class="line">  <span class="comment">#bodyEnd: source/_data/body-end.swig</span></span><br><span class="line">  <span class="comment">#variable: source/_data/variables.styl</span></span><br><span class="line">  <span class="comment">#mixin: source/_data/mixins.styl</span></span><br><span class="line">  <span class="symbol">style:</span> source/_data/styles.styl</span><br></pre></td></tr></table></figure><p> &emsp;&emsp;将最后一个的注释取消，然后到blog的source文件夹下(<strong>注意，是blog的source，不是next的source</strong>)创建这个文件，将设置背景图片的css放进去，<br> <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line"> <span class="attribute">background-image</span>:<span class="built_in">url</span>(/images/header.jpg); </span><br><span class="line"> <span class="attribute">background-repeat</span>: no-repeat; </span><br><span class="line"> <span class="attribute">background-attachment</span>:fixed; </span><br><span class="line"> <span class="attribute">background-position</span>:<span class="number">50%</span> <span class="number">50%</span>;</span><br><span class="line"> <span class="attribute">background-size</span>:cover;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>&emsp;&emsp;还有注意将图片放在next的source的images中，至于为什么这么迷我也不清楚….<br>&emsp;&emsp;这里顺便说一下，设置背景透明度的css也直接放在这个文件中就可以了，透明度的效果可以根据自己的喜好调节，1是不透明，0是全透明，我是小透明，不是很聪明。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="selector-class">.main-inner</span> &#123; </span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">0.9</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="点击头像回到主页"><a href="#点击头像回到主页" class="headerlink" title="点击头像回到主页"></a>点击头像回到主页</h2><p>&emsp;&emsp;确实哈，鼠标放在头像上就想去点，虽然卵用不大，但这个功能必须得有啊。<br>&emsp;&emsp;网上教程用不了，在sidebar.swig文件中没有头像的标签，于是我就把附近的文件找了找，反正思路就是加个跳转，管他在哪里，只要能找到，加上看他跳不跳。<br>&emsp;&emsp;终于，在\next\layout_partials\sidebar下找到了site-overview.swig文件，找到</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">class</span>=<span class="string">"site-author-image"</span> <span class="attr">itemprop</span>=<span class="string">"image"</span> <span class="attr">alt</span>=<span class="string">"&#123;&#123; author &#125;&#125;"</span> <span class="attr">src</span>=<span class="string">"&#123;&#123; url_for(theme.avatar.url or theme.images + '/header.jpg') &#125;&#125;"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>&emsp;&emsp;在这段代码的开始前面和结束后面分别加上第一行和第二行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><p>&emsp;&emsp;记得将img标签的src改成自己头像路径。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hexo-g-执行后不生成index-html&quot;&gt;&lt;a href=&quot;#hexo-g-执行后不生成index-html&quot; class=&quot;headerlink&quot; title=&quot;hexo g 执行后不生成index.html&quot;&gt;&lt;/a&gt;hexo g 执行后不生成inde
      
    
    </summary>
    
    
      <category term="hexo博客搭建" scheme="https://december2580.github.io/categories/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="hexo" scheme="https://december2580.github.io/tags/hexo/"/>
    
      <category term="next主题美化" scheme="https://december2580.github.io/tags/next%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/"/>
    
  </entry>
  
</feed>
